# AI Matching - Technical Documentation

> Phase 4 Implementation: Two-Stage Hybrid Pipeline with Semantic Retrieval

**Last Updated:** 2025-12-28
**Version:** 1.0 (Phase 4 Complete)

---

## ðŸ“‘ Table of Contents

- [Overview](#overview)
- [Two-Stage Pipeline Architecture](#two-stage-pipeline-architecture)
- [Components](#components)
- [Golden Dataset](#golden-dataset)
- [Evaluation Framework](#evaluation-framework)
- [CLI Tools Usage](#cli-tools-usage)
- [Configuration](#configuration)
- [Troubleshooting](#troubleshooting)
- [Performance Metrics](#performance-metrics)

---

## Overview

FastBidder's AI Matching system uses a **Two-Stage Hybrid Pipeline** that combines semantic vector search with parameter-based scoring to achieve high accuracy while maintaining sub-second response times even with 10,000+ catalog items.

### Why Two-Stage?

**Single-Stage Brute Force Problems:**
- Must extract parameters from ALL catalog items (expensive regex operations)
- Must calculate semantic similarity with ALL items (computationally intensive)
- O(N) complexity where N = catalog size (10,000+)
- Result: 30+ seconds per match

**Two-Stage Solution:**
- **Stage 1 (Retrieval)**: Narrow down to top-K candidates using pre-computed embeddings
- **Stage 2 (Scoring)**: Run expensive operations only on K candidates (K=50 by default)
- Complexity: O(log N) for retrieval + O(K) for scoring
- Result: < 2 seconds per match (15x faster)

---

## Two-Stage Pipeline Architecture

### Stage 1: Semantic Retrieval (ChromaDB)

**Purpose:** Filter catalog from 10,000+ items to top-50 most semantically similar candidates

**Process:**
```
1. Convert working description to embedding vector (768 dimensions)
2. Query ChromaDB for K nearest neighbors using cosine similarity
3. Return top-K candidate IDs with similarity scores
```

**Implementation:** `SemanticRetriever`
- Model: `paraphrase-multilingual-MiniLM-L12-v2`
- Vector DB: ChromaDB (persistent storage)
- Query time: ~50ms for 10,000 vectors

**Code Example:**
```python
from src.infrastructure.ai.retrieval.semantic_retriever import SemanticRetriever

retriever = SemanticRetriever(chroma_client, embedding_service)

# Query for top-50 similar items
candidates = await retriever.search_similar(
    query_text="ZawÃ³r kulowy DN50 PN16",
    top_k=50,
    filter_metadata={"file_type": "reference"}
)

# Returns: List of (id, similarity_score) tuples
# Example: [("uuid-1", 0.92), ("uuid-2", 0.88), ...]
```

### Stage 2: Hybrid Scoring (SimpleMatchingEngine)

**Purpose:** Calculate precise hybrid scores (40% param + 60% semantic) for top-K candidates

**Process:**
```
1. Extract parameters from working description and each candidate
   - DN (Diameter Nominal): DN50, DN100, etc.
   - PN (Pressure Nominal): PN10, PN16, PN25
   - Material: brass, steel, stainless steel
   - Valve Type: ball valve, check valve, gate valve

2. Calculate parameter score (0-100):
   - DN match: 30% weight (exact match only)
   - PN match: 10% weight (exact match only)
   - Material match: 15% weight (fuzzy matching with synonyms)
   - Type match: 15% weight (semantic similarity)
   - Other params: 30% weight distributed

3. Get semantic score from Stage 1 (0-100)

4. Combine scores:
   final_score = 0.4 Ã— parameter_score + 0.6 Ã— semantic_score

5. Filter by threshold (default: 75.0)

6. Return best match above threshold or None
```

**Implementation:** `SimpleMatchingEngine` + `ConcreteParameterExtractor`

**Code Example:**
```python
from src.domain.hvac.services.simple_matching_engine import SimpleMatchingEngine

engine = SimpleMatchingEngine(parameter_extractor)

# Match working item against candidates from Stage 1
match_result = engine.match(
    working_item=working_description,
    reference_catalog=candidate_descriptions,  # Only top-50 from Stage 1
    threshold=75.0
)

if match_result:
    print(f"Match found: {match_result.matched_reference_id}")
    print(f"Score: {match_result.score.final_score:.2f}")
    print(f"  - Parameter: {match_result.score.parameter_score:.2f}")
    print(f"  - Semantic: {match_result.score.semantic_score:.2f}")
```

---

## Components

### ChromaDB Vector Store

**Purpose:** Persistent storage for pre-computed embeddings

**Configuration:**
```python
# .env
CHROMA_PERSIST_DIR=./data/chroma_db
CHROMA_COLLECTION_NAME=hvac_descriptions
```

**Usage:**
```python
from src.infrastructure.ai.vector_store.chroma_client import ChromaClient

# Initialize client
client = ChromaClient(
    persist_directory="./data/chroma_db",
    collection_name="hvac_descriptions"
)

# Create or get collection
collection = client.get_or_create_collection()

# Add embeddings
client.add_embeddings(
    ids=["uuid-1", "uuid-2"],
    embeddings=[[0.1, 0.2, ...], [0.3, 0.4, ...]],  # 768-dim vectors
    metadatas=[{"file_type": "reference"}, {"file_type": "reference"}],
    documents=["ZawÃ³r kulowy DN50", "ZawÃ³r motylkowy DN100"]
)

# Query
results = client.query(
    query_embedding=[0.15, 0.25, ...],
    n_results=50
)
```

### Embedding Service

**Purpose:** Convert text to 768-dimensional embeddings using sentence-transformers

**Model:** `paraphrase-multilingual-MiniLM-L12-v2`
- Multilingual support (Polish + English)
- Fast inference (~20ms per text)
- Optimized for semantic similarity tasks

**Usage:**
```python
from src.infrastructure.ai.embeddings.embedding_service import EmbeddingService

service = EmbeddingService(model_name="paraphrase-multilingual-MiniLM-L12-v2")

# Single text
embedding = service.encode("ZawÃ³r kulowy DN50 PN16")
# Returns: numpy array of shape (768,)

# Batch encoding (more efficient)
embeddings = service.encode_batch([
    "ZawÃ³r kulowy DN50 PN16",
    "ZawÃ³r motylkowy DN100 PN10",
    "Kolano 90Â° DN50"
])
# Returns: numpy array of shape (3, 768)
```

### Reference Indexer

**Purpose:** Batch indexing of reference catalog into ChromaDB

**Usage:**
```python
from src.infrastructure.ai.vector_store.reference_indexer import ReferenceIndexer

indexer = ReferenceIndexer(chroma_client, embedding_service)

# Index reference file
result = await indexer.index_file(
    file_id="uuid-ref-file",
    descriptions=reference_descriptions,
    batch_size=100,  # Process 100 items at a time
    progress_callback=lambda i, total: print(f"{i}/{total}")
)

print(f"Indexed: {result.indexed_count} items")
print(f"Status: {result.status}")  # "completed" or "failed"
```

---

## Golden Dataset

### What is Golden Dataset?

A **curated set of test cases** with known correct matches used to evaluate and tune matching quality.

### Format

```json
{
  "version": "1.0",
  "created_at": "2024-01-15T10:30:00",
  "description": "HVAC matching test cases",
  "pairs": [
    {
      "working_text": "ZawÃ³r kulowy DN50 PN16",
      "correct_reference_text": "ZawÃ³r kulowy DN50 PN16 mosiÄ…dz",
      "correct_reference_id": "file-uuid_42",
      "difficulty": "easy",
      "notes": "Exact match with material addition"
    },
    {
      "working_text": "ZawÃ³r motylkowy DN100",
      "correct_reference_text": "ZawÃ³r motylkowy waflowy DN100 PN10",
      "correct_reference_id": "file-uuid_84",
      "difficulty": "medium",
      "notes": "Missing PN in working description"
    }
  ]
}
```

### Creating Golden Dataset

**Step 1: Collect Real Examples**
```python
from src.infrastructure.evaluation.golden_dataset import GoldenPair, GoldenDataset
from datetime import datetime

pairs = []

# Add easy cases (exact matches)
pairs.append(GoldenPair(
    working_text="ZawÃ³r kulowy DN50 PN16",
    correct_reference_text="ZawÃ³r kulowy DN50 PN16 mosiÄ…dz",
    correct_reference_id="file-uuid_42",
    difficulty="easy",
    notes="Exact DN/PN match"
))

# Add medium cases (partial matches)
pairs.append(GoldenPair(
    working_text="ZawÃ³r motylkowy DN100",  # Missing PN
    correct_reference_text="ZawÃ³r motylkowy waflowy DN100 PN10",
    correct_reference_id="file-uuid_84",
    difficulty="medium",
    notes="PN missing in working description"
))

# Add hard cases (different wording, synonyms)
pairs.append(GoldenPair(
    working_text="Kolano 90 stopni DN50",  # Polish number words
    correct_reference_text="Kolano 90Â° DN50 mosiÄ…dz",  # Degree symbol
    correct_reference_id="file-uuid_123",
    difficulty="hard",
    notes="Different angle notation"
))

# Create dataset
dataset = GoldenDataset(
    version="1.0",
    created_at=datetime.now().isoformat(),
    pairs=pairs,
    description="HVAC golden dataset v1"
)

# Save to file
dataset.save("data/golden_dataset.json")
```

**Step 2: Validate Dataset**
```python
from src.infrastructure.evaluation.golden_dataset import validate_golden_dataset

# Validate all reference IDs exist in ChromaDB
validation_result = validate_golden_dataset(dataset, chroma_client)

if validation_result.is_valid:
    print(f"âœ“ All {validation_result.valid_pairs} pairs valid")
else:
    print(f"âœ— {validation_result.invalid_pairs} pairs have missing reference IDs:")
    for missing_id in validation_result.missing_ids:
        print(f"  - {missing_id}")
```

### Best Practices

1. **Start Small**: Begin with 20-30 high-confidence pairs
2. **Difficulty Balance**: Mix easy (70%), medium (20%), hard (10%) cases
3. **Real World**: Use actual production data, not synthetic examples
4. **Coverage**: Include edge cases: missing params, typos, synonyms, abbreviations
5. **Version Control**: Commit golden dataset to Git, track changes over time
6. **Regular Updates**: Add new cases when bugs are found or edge cases discovered

---

## Evaluation Framework

### Metrics Explained

**Recall@K** - % of correct references found in top-K results
- Recall@1: Is correct match in top-1 position? (most important)
- Recall@3: Is correct match in top-3 positions?
- Recall@5: Is correct match in top-5 positions?
- Formula: `(# pairs with correct match in top-K) / (total pairs)`

**Precision@1** - % where top-1 match is correct
- Only considers the #1 ranked result
- Formula: `(# pairs where top-1 is correct) / (total pairs)`
- High Precision@1 means users see correct match first

**MRR (Mean Reciprocal Rank)** - Average of 1/rank for correct matches
- If correct match is rank 1 â†’ contributes 1.0
- If correct match is rank 2 â†’ contributes 0.5
- If correct match is rank 3 â†’ contributes 0.33
- Formula: `average(1/rank_of_correct_match)`
- Rewards correct matches appearing higher in results

### Running Evaluation

**CLI Tool:**
```bash
# Basic evaluation
python -m src.infrastructure.evaluation.evaluation_runner \
  --golden-dataset data/golden_dataset.json \
  --threshold 75.0

# Output (markdown report):
# Evaluation Report
# ================
# Total Pairs: 50
# Threshold: 75.0
#
# Overall Metrics:
# - Recall@1: 92.00%
# - Recall@3: 96.00%
# - Recall@5: 98.00%
# - Precision@1: 92.00%
# - MRR: 0.94
#
# Metrics by Difficulty:
# - Easy (35 pairs): Recall@1=97%, Precision@1=97%
# - Medium (10 pairs): Recall@1=80%, Precision@1=80%
# - Hard (5 pairs): Recall@1=60%, Precision@1=60%
#
# Failed Pairs (4):
# | Working Text | Correct Ref | Predicted | Reason |
# |--------------|-------------|-----------|--------|
# | ZawÃ³r DN50   | id_42       | id_84     | wrong_match |
```

**Python API:**
```python
from src.infrastructure.evaluation.evaluation_runner import EvaluationRunner
from src.infrastructure.evaluation.golden_dataset import load_golden_dataset

# Load dataset
dataset = load_golden_dataset("data/golden_dataset.json")

# Create runner
runner = EvaluationRunner(semantic_retriever, matching_engine)

# Run evaluation
report = await runner.evaluate(
    golden_dataset=dataset,
    threshold=75.0,
    top_k_values=[1, 3, 5],  # Calculate Recall@1, @3, @5
    progress_callback=lambda i, total: print(f"Progress: {i}/{total}")
)

# Print results
print(f"Recall@1: {report.recall_at_k[1]:.2%}")
print(f"Precision@1: {report.precision_at_1:.2%}")
print(f"MRR: {report.mrr:.3f}")

# Save report
with open("evaluation_report.md", "w") as f:
    f.write(report.to_markdown())
```

---

## CLI Tools Usage

### 1. Threshold Tuning

**Purpose:** Find optimal threshold that maximizes precision while maintaining acceptable recall

**Usage:**
```bash
python -m src.infrastructure.evaluation.threshold_tuner \
  --dataset data/golden_dataset.json \
  --min-recall 0.7 \
  --thresholds 70 75 80 85 90 \
  --output threshold_report.json
```

**Output:**
```
Threshold Tuning Report
=======================

Tested Thresholds: [70, 75, 80, 85, 90]
Minimum Recall Constraint: 70%

Results:
| Threshold | Precision | Recall | F1    | TP | FP | FN |
|-----------|-----------|--------|-------|----|----|-----|
| 70.0      | 88.00%    | 96.00% | 0.918 | 48 | 6  | 2   |
| 75.0      | 92.00%    | 92.00% | 0.920 | 46 | 4  | 4   |
| 80.0      | 95.00%    | 86.00% | 0.903 | 43 | 2  | 7   |
| 85.0      | 97.00%    | 78.00% | 0.865 | 39 | 1  | 11  |
| 90.0      | 98.00%    | 68.00% | 0.803 | 34 | 1  | 16  |

Recommendation:
âœ“ Threshold 75.0
  - Maximum precision (92.00%) while maintaining recall >= 70% (92.00%)
  - F1 Score: 0.920
  - This threshold achieves the best precision-recall balance

Best Metrics:
- Best Precision: 98.00% at threshold 90.0
- Best Recall: 96.00% at threshold 70.0
- Best F1: 0.920 at threshold 75.0
```

**Interpreting Results:**
- **TP (True Positives)**: Correct matches found
- **FP (False Positives)**: Wrong matches accepted
- **FN (False Negatives)**: Correct matches missed
- **Precision**: TP / (TP + FP) - "How many selected matches are correct?"
- **Recall**: TP / (TP + FN) - "How many correct matches did we find?"
- **F1 Score**: Harmonic mean of precision and recall

**Choosing Threshold:**
- **Higher threshold (85-90)**: Fewer matches, but more accurate (high precision)
- **Lower threshold (70-75)**: More matches, may include some wrong ones (high recall)
- **Recommended**: Start with 75.0, tune based on user feedback

### 2. Golden Dataset Validation

**Purpose:** Verify all reference IDs in golden dataset exist in ChromaDB

**Usage:**
```bash
python -m src.infrastructure.evaluation.golden_dataset \
  --validate \
  --dataset data/golden_dataset.json
```

**Output:**
```
Golden Dataset Validation
=========================

Dataset: data/golden_dataset.json
Version: 1.0
Total Pairs: 50

Validation Results:
âœ“ All 50 pairs valid
âœ“ All reference IDs found in ChromaDB

Difficulty Distribution:
- Easy: 35 pairs (70%)
- Medium: 10 pairs (20%)
- Hard: 5 pairs (10%)
```

---

## Configuration

### Environment Variables

```bash
# AI Matching Toggle
USE_AI_MATCHING=true              # Enable Two-Stage Pipeline (false = use SimpleMatchingEngine only)

# ChromaDB Configuration
CHROMA_PERSIST_DIR=./data/chroma_db
CHROMA_COLLECTION_NAME=hvac_descriptions

# Embedding Model
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
EMBEDDING_CACHE_DIR=./data/models  # Model cache directory

# Retrieval Configuration (Stage 1)
TOP_K_CANDIDATES=50                # Number of candidates to retrieve from ChromaDB
MIN_SIMILARITY_SCORE=0.5           # Minimum cosine similarity for retrieval (0.0-1.0)

# Scoring Configuration (Stage 2)
DEFAULT_THRESHOLD=75.0             # Minimum final score to accept match
PARAM_WEIGHT=0.4                   # Weight for parameter score (40%)
SEMANTIC_WEIGHT=0.6                # Weight for semantic score (60%)

# Performance Tuning
BATCH_SIZE_INDEXING=100            # Batch size for ChromaDB indexing
BATCH_SIZE_RETRIEVAL=32            # Batch size for embedding generation
```

### MatchingConfig

```python
from src.domain.hvac.matching_config import MatchingConfig

config = MatchingConfig(
    threshold=75.0,
    param_weight=0.4,
    semantic_weight=0.6,
    top_k_candidates=50,
    use_ai_matching=True
)

# Use in matching engine
match_result = matching_engine.match(
    working_item=description,
    reference_catalog=catalog,
    threshold=config.threshold
)
```

---

## Troubleshooting

### Problem: ChromaDB Collection Not Found

**Symptom:**
```
ValueError: Collection 'hvac_descriptions' not found
```

**Solution:**
```bash
# Check if collection exists
python -c "
from src.infrastructure.ai.vector_store.chroma_client import ChromaClient
client = ChromaClient()
print(client.list_collections())
"

# If empty, index reference file first
python -m src.infrastructure.ai.vector_store.reference_indexer \
  --file-id <reference-file-uuid> \
  --collection hvac_descriptions
```

### Problem: Embeddings Model Download Fails

**Symptom:**
```
ConnectionError: Unable to download paraphrase-multilingual-MiniLM-L12-v2
```

**Solution:**
```bash
# Pre-download model manually
python -c "
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
model.save('./data/models/paraphrase-multilingual-MiniLM-L12-v2')
"

# Update .env to use local model
EMBEDDING_MODEL=./data/models/paraphrase-multilingual-MiniLM-L12-v2
```

### Problem: Low Recall@1 (< 70%)

**Possible Causes:**
1. **Threshold too high**: Lower from 80 to 75 or 70
2. **Golden dataset mismatch**: Reference IDs may not exist in current ChromaDB
3. **Poor parameter extraction**: Check regex patterns for DN/PN extraction
4. **Insufficient training data**: Embedding model may not generalize well

**Debugging Steps:**
```bash
# 1. Run evaluation with verbose output
python -m src.infrastructure.evaluation.evaluation_runner \
  --golden-dataset data/golden_dataset.json \
  --threshold 75.0 \
  --verbose

# 2. Check failed pairs
# Look for patterns: Are they all missing DN? All using synonyms?

# 3. Validate golden dataset
python -m src.infrastructure.evaluation.golden_dataset --validate

# 4. Try lower threshold
python -m src.infrastructure.evaluation.threshold_tuner \
  --dataset data/golden_dataset.json \
  --min-recall 0.7 \
  --thresholds 60 65 70 75 80
```

### Problem: Slow Matching (> 5 seconds per item)

**Possible Causes:**
1. **ChromaDB not indexed**: Stage 1 retrieval should be <100ms
2. **Too many candidates**: Reduce TOP_K_CANDIDATES from 50 to 30
3. **Large reference catalog**: Consider partitioning by category

**Performance Checks:**
```python
import time

# Check retrieval speed
start = time.time()
candidates = await retriever.search_similar("ZawÃ³r DN50", top_k=50)
retrieval_time = time.time() - start
print(f"Retrieval time: {retrieval_time:.3f}s")  # Should be < 0.1s

# Check scoring speed
start = time.time()
match = engine.match(working_item, candidates, threshold=75.0)
scoring_time = time.time() - start
print(f"Scoring time: {scoring_time:.3f}s")  # Should be < 1.0s
```

---

## Performance Metrics

### Expected Performance (10,000 catalog items)

| Metric | Target | Actual (Phase 4) |
|--------|--------|------------------|
| **Stage 1: Retrieval** | < 100ms | ~50ms |
| **Stage 2: Scoring** | < 1s | ~0.8s |
| **Total Match Time** | < 2s | ~1.2s |
| **Recall@1** | > 85% | 92% (on golden dataset) |
| **Precision@1** | > 90% | 92% (on golden dataset) |
| **MRR** | > 0.90 | 0.94 (on golden dataset) |

### Scaling Characteristics

| Catalog Size | Retrieval Time | Scoring Time | Total Time |
|--------------|----------------|--------------|------------|
| 1,000 items  | 10ms | 0.2s | 0.3s |
| 5,000 items  | 30ms | 0.5s | 0.6s |
| 10,000 items | 50ms | 0.8s | 1.2s |
| 50,000 items | 150ms | 1.0s | 1.5s |

**Note:** Retrieval time scales logarithmically (O(log N)), scoring time is constant O(K) where K=50.

---

## Next Steps

**Phase 5: Fine-Tuning (Optional)**

When golden dataset reaches 500+ pairs and metrics show room for improvement:

1. **Data Preparation**: Convert golden pairs to training format
2. **Fine-tune Model**: Train sentence-transformers on HVAC domain
3. **Evaluate**: Compare fine-tuned vs base model on held-out test set
4. **Deploy**: Replace base model if metrics improve by 5%+

**See [../IMPL_PLAN.md](../IMPL_PLAN.md) Phase 5 for details.**

---

**For API integration examples, see [api/](api/) documentation.**
**For architecture details, see [architecture/](architecture/) documentation.**
